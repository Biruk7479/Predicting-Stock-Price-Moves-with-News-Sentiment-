{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caafa8bc",
   "metadata": {
    "vscode": {
     "languageId": "code"
    }
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# Add src directory to path\n",
    "sys.path.append('../src')\n",
    "\n",
    "# Import custom modules\n",
    "from data_loader import DataLoader, load_and_prepare_data\n",
    "from eda import EDAAnalyzer\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76aed944",
   "metadata": {},
   "source": [
    "## 1. Load the Data\n",
    "\n",
    "We'll start by loading a sample of the dataset to perform EDA. Given the large size (1.4M+ rows), we'll work with a manageable subset initially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3caf9554",
   "metadata": {
    "vscode": {
     "languageId": "code"
    }
   },
   "outputs": [],
   "source": [
    "# Define data path\n",
    "DATA_PATH = '../Data/newsData/raw_analyst_ratings.csv'\n",
    "\n",
    "# Initialize data loader\n",
    "loader = DataLoader(DATA_PATH)\n",
    "\n",
    "# Load data - start with 100k rows for initial analysis\n",
    "print(\"Loading data...\")\n",
    "df = loader.load_data(nrows=100000)\n",
    "\n",
    "print(f\"\\nDataset shape: {df.shape}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95514cb9",
   "metadata": {},
   "source": [
    "## 2. Data Overview and Basic Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a532d57",
   "metadata": {
    "vscode": {
     "languageId": "code"
    }
   },
   "outputs": [],
   "source": [
    "# Display data info\n",
    "print(\"=== Dataset Information ===\")\n",
    "print(df.info())\n",
    "\n",
    "print(\"\\n=== Missing Values ===\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "print(\"\\n=== Data Types ===\")\n",
    "print(df.dtypes)\n",
    "\n",
    "print(\"\\n=== Sample Records ===\")\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7f9a8d",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba86f7b",
   "metadata": {
    "vscode": {
     "languageId": "code"
    }
   },
   "outputs": [],
   "source": [
    "# Preprocess the data\n",
    "print(\"Preprocessing data...\")\n",
    "df = loader.preprocess()\n",
    "\n",
    "print(f\"\\nDataset shape after preprocessing: {df.shape}\")\n",
    "print(f\"\\nNew columns added:\")\n",
    "print(df.columns.tolist())\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35affe84",
   "metadata": {},
   "source": [
    "## 4. Descriptive Statistics\n",
    "\n",
    "Let's examine basic statistics for textual lengths and other numerical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4859e60c",
   "metadata": {
    "vscode": {
     "languageId": "code"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize EDA Analyzer\n",
    "eda = EDAAnalyzer(df)\n",
    "\n",
    "# Get descriptive statistics\n",
    "stats = eda.descriptive_statistics()\n",
    "\n",
    "print(\"=== DESCRIPTIVE STATISTICS ===\\n\")\n",
    "print(f\"Total Articles: {stats['total_articles']:,}\")\n",
    "print(f\"Unique Stocks: {stats['unique_stocks']:,}\")\n",
    "print(f\"Unique Publishers: {stats['unique_publishers']:,}\")\n",
    "print(f\"\\nDate Range:\")\n",
    "print(f\"  From: {stats['date_range'][0]}\")\n",
    "print(f\"  To: {stats['date_range'][1]}\")\n",
    "\n",
    "print(\"\\n=== Headline Length Statistics ===\")\n",
    "print(stats['headline_length_stats'])\n",
    "\n",
    "print(\"\\n=== Word Count Statistics ===\")\n",
    "print(stats['word_count_stats'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16097308",
   "metadata": {
    "vscode": {
     "languageId": "code"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize headline length and word count distributions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Headline length distribution\n",
    "axes[0, 0].hist(df['headline_length'], bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[0, 0].set_title('Distribution of Headline Length', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Characters')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].axvline(df['headline_length'].mean(), color='red', linestyle='--', label='Mean')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# Word count distribution\n",
    "axes[0, 1].hist(df['word_count'], bins=50, edgecolor='black', alpha=0.7, color='orange')\n",
    "axes[0, 1].set_title('Distribution of Word Count', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Words')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].axvline(df['word_count'].mean(), color='red', linestyle='--', label='Mean')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# Box plots\n",
    "axes[1, 0].boxplot(df['headline_length'], vert=False)\n",
    "axes[1, 0].set_title('Headline Length - Box Plot', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Characters')\n",
    "\n",
    "axes[1, 1].boxplot(df['word_count'], vert=False)\n",
    "axes[1, 1].set_title('Word Count - Box Plot', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Words')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n=== Key Insights ===\")\n",
    "print(f\"Average headline length: {df['headline_length'].mean():.1f} characters\")\n",
    "print(f\"Average word count: {df['word_count'].mean():.1f} words\")\n",
    "print(f\"Median headline length: {df['headline_length'].median():.0f} characters\")\n",
    "print(f\"Median word count: {df['word_count'].median():.0f} words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49c78f0",
   "metadata": {},
   "source": [
    "## 5. Publisher Analysis\n",
    "\n",
    "Identify which publishers are most active and their coverage patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0d7337",
   "metadata": {
    "vscode": {
     "languageId": "code"
    }
   },
   "outputs": [],
   "source": [
    "# Get publisher statistics\n",
    "publisher_stats = eda.publisher_analysis()\n",
    "\n",
    "print(\"=== TOP 20 PUBLISHERS BY ARTICLE COUNT ===\\n\")\n",
    "print(publisher_stats.head(20))\n",
    "\n",
    "print(\"\\n=== Publisher Summary ===\")\n",
    "print(f\"Total publishers: {len(publisher_stats)}\")\n",
    "print(f\"Most active publisher: {publisher_stats.index[0]} ({publisher_stats.iloc[0]['article_count']:,} articles)\")\n",
    "print(f\"Average articles per publisher: {publisher_stats['article_count'].mean():.1f}\")\n",
    "print(f\"Median articles per publisher: {publisher_stats['article_count'].median():.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6194e6",
   "metadata": {
    "vscode": {
     "languageId": "code"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize top publishers\n",
    "fig = eda.plot_publisher_distribution(top_n=25, figsize=(14, 10))\n",
    "plt.show()\n",
    "\n",
    "# Publisher concentration analysis\n",
    "top_10_pct = (publisher_stats.head(10)['article_count'].sum() / publisher_stats['article_count'].sum()) * 100\n",
    "top_20_pct = (publisher_stats.head(20)['article_count'].sum() / publisher_stats['article_count'].sum()) * 100\n",
    "\n",
    "print(f\"\\n=== Publisher Concentration ===\")\n",
    "print(f\"Top 10 publishers account for: {top_10_pct:.1f}% of all articles\")\n",
    "print(f\"Top 20 publishers account for: {top_20_pct:.1f}% of all articles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3d9e72",
   "metadata": {
    "vscode": {
     "languageId": "code"
    }
   },
   "outputs": [],
   "source": [
    "# Identify publisher domains\n",
    "df['publisher_domain'] = df['publisher'].apply(\n",
    "    lambda x: x.split('@')[1] if '@' in str(x) else 'Named Author'\n",
    ")\n",
    "\n",
    "domain_counts = df['publisher_domain'].value_counts().head(15)\n",
    "\n",
    "print(\"=== TOP PUBLISHER DOMAINS ===\\n\")\n",
    "print(domain_counts)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(12, 6))\n",
    "domain_counts.plot(kind='bar', color='skyblue', edgecolor='black')\n",
    "plt.title('Top 15 Publisher Domains', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Domain')\n",
    "plt.ylabel('Number of Articles')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ace9a62",
   "metadata": {},
   "source": [
    "## 6. Stock Analysis\n",
    "\n",
    "Analyze which stocks receive the most coverage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f9f389",
   "metadata": {
    "vscode": {
     "languageId": "code"
    }
   },
   "outputs": [],
   "source": [
    "# Get stock statistics\n",
    "stock_stats = eda.stock_analysis()\n",
    "\n",
    "print(\"=== TOP 30 STOCKS BY ARTICLE COUNT ===\\n\")\n",
    "print(stock_stats.head(30))\n",
    "\n",
    "print(\"\\n=== Stock Coverage Summary ===\")\n",
    "print(f\"Total unique stocks: {len(stock_stats)}\")\n",
    "print(f\"Most covered stock: {stock_stats.index[0]} ({stock_stats.iloc[0]['article_count']:,} articles)\")\n",
    "print(f\"Average articles per stock: {stock_stats['article_count'].mean():.1f}\")\n",
    "print(f\"Median articles per stock: {stock_stats['article_count'].median():.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6727ed",
   "metadata": {
    "vscode": {
     "languageId": "code"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize top stocks\n",
    "fig = eda.plot_stock_distribution(top_n=40, figsize=(14, 12))\n",
    "plt.show()\n",
    "\n",
    "# Stock coverage concentration\n",
    "top_50_stocks_pct = (stock_stats.head(50)['article_count'].sum() / stock_stats['article_count'].sum()) * 100\n",
    "top_100_stocks_pct = (stock_stats.head(100)['article_count'].sum() / stock_stats['article_count'].sum()) * 100\n",
    "\n",
    "print(f\"\\n=== Stock Coverage Concentration ===\")\n",
    "print(f\"Top 50 stocks account for: {top_50_stocks_pct:.1f}% of all articles\")\n",
    "print(f\"Top 100 stocks account for: {top_100_stocks_pct:.1f}% of all articles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e27ee27",
   "metadata": {},
   "source": [
    "## 7. Time Series Analysis\n",
    "\n",
    "Analyze publication patterns over time - daily, hourly, and day-of-week trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54269487",
   "metadata": {
    "vscode": {
     "languageId": "code"
    }
   },
   "outputs": [],
   "source": [
    "# Get time series statistics\n",
    "time_stats = eda.time_series_analysis()\n",
    "\n",
    "print(\"=== TIME SERIES ANALYSIS ===\\n\")\n",
    "\n",
    "print(\"Daily article statistics:\")\n",
    "print(f\"  Average: {time_stats['daily'].mean():.1f} articles/day\")\n",
    "print(f\"  Median: {time_stats['daily'].median():.0f} articles/day\")\n",
    "print(f\"  Max: {time_stats['daily'].max():,} articles (on {time_stats['daily'].idxmax()})\")\n",
    "print(f\"  Min: {time_stats['daily'].min():,} articles (on {time_stats['daily'].idxmin()})\")\n",
    "\n",
    "print(\"\\nHourly distribution (top 5 hours):\")\n",
    "print(time_stats['hourly'].sort_values(ascending=False).head())\n",
    "\n",
    "print(\"\\nDay of week distribution:\")\n",
    "day_names = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "for day_idx, count in time_stats['day_of_week'].items():\n",
    "    print(f\"  {day_names[day_idx]}: {count:,} articles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4de96a5",
   "metadata": {
    "vscode": {
     "languageId": "code"
    }
   },
   "outputs": [],
   "source": [
    "# Comprehensive time series visualization\n",
    "fig = eda.plot_time_series(figsize=(16, 12))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ccfb99",
   "metadata": {
    "vscode": {
     "languageId": "code"
    }
   },
   "outputs": [],
   "source": [
    "# Analyze monthly trends\n",
    "monthly_trend = df.groupby([df['year'], df['month']]).size()\n",
    "monthly_trend.index = monthly_trend.index.map(lambda x: f\"{x[0]}-{x[1]:02d}\")\n",
    "\n",
    "plt.figure(figsize=(16, 6))\n",
    "monthly_trend.plot(kind='line', marker='o', linewidth=2, markersize=4)\n",
    "plt.title('Monthly Article Publication Trend', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Year-Month')\n",
    "plt.ylabel('Number of Articles')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n=== Monthly Trend Insights ===\")\n",
    "print(f\"Highest publication month: {monthly_trend.idxmax()} with {monthly_trend.max():,} articles\")\n",
    "print(f\"Lowest publication month: {monthly_trend.idxmin()} with {monthly_trend.min():,} articles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfd2f54",
   "metadata": {
    "vscode": {
     "languageId": "code"
    }
   },
   "outputs": [],
   "source": [
    "# Analyze publication time patterns\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "# Hourly heatmap by day of week\n",
    "hourly_dow = df.groupby(['dayofweek', 'hour']).size().unstack(fill_value=0)\n",
    "sns.heatmap(hourly_dow, cmap='YlOrRd', ax=axes[0], cbar_kws={'label': 'Article Count'})\n",
    "axes[0].set_title('Publication Heatmap: Hour vs Day of Week', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Hour of Day')\n",
    "axes[0].set_ylabel('Day of Week')\n",
    "axes[0].set_yticklabels(day_names, rotation=0)\n",
    "\n",
    "# Weekend vs Weekday\n",
    "df['is_weekend'] = df['dayofweek'].isin([5, 6])\n",
    "weekend_counts = df.groupby('is_weekend').size()\n",
    "axes[1].pie(weekend_counts, labels=['Weekday', 'Weekend'], autopct='%1.1f%%', \n",
    "           startangle=90, colors=['#ff9999', '#66b3ff'])\n",
    "axes[1].set_title('Weekend vs Weekday Publication', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n=== Publication Timing Insights ===\")\n",
    "print(f\"Weekday articles: {weekend_counts[False]:,} ({weekend_counts[False]/len(df)*100:.1f}%)\")\n",
    "print(f\"Weekend articles: {weekend_counts[True]:,} ({weekend_counts[True]/len(df)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20af9f70",
   "metadata": {},
   "source": [
    "## 8. Text Analysis and Topic Modeling\n",
    "\n",
    "Extract common keywords and themes from headlines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4867bce6",
   "metadata": {
    "vscode": {
     "languageId": "code"
    }
   },
   "outputs": [],
   "source": [
    "# Extract top keywords\n",
    "print(\"Extracting keywords from headlines...\")\n",
    "keywords = eda.extract_keywords(n_keywords=50)\n",
    "\n",
    "print(\"\\n=== TOP 50 KEYWORDS IN HEADLINES ===\\n\")\n",
    "for i, (word, count) in enumerate(keywords, 1):\n",
    "    print(f\"{i:2d}. {word:20s} - {count:,} occurrences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bd0753",
   "metadata": {
    "vscode": {
     "languageId": "code"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize top keywords\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Create word frequency dictionary for word cloud\n",
    "word_freq = dict(keywords[:100])\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "\n",
    "# Word cloud\n",
    "plt.subplot(1, 2, 1)\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white', \n",
    "                     colormap='viridis', max_words=100).generate_from_frequencies(word_freq)\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Word Cloud of Top Keywords', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Bar chart of top 30 keywords\n",
    "plt.subplot(1, 2, 2)\n",
    "top_30_words = keywords[:30]\n",
    "words, counts = zip(*top_30_words)\n",
    "plt.barh(range(len(words)), counts, color='teal')\n",
    "plt.yticks(range(len(words)), words)\n",
    "plt.xlabel('Frequency')\n",
    "plt.title('Top 30 Keywords Frequency', fontsize=16, fontweight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c48a45",
   "metadata": {
    "vscode": {
     "languageId": "code"
    }
   },
   "outputs": [],
   "source": [
    "# Identify common financial terms\n",
    "financial_terms = {\n",
    "    'price': ['price', 'target', 'pt'],\n",
    "    'earnings': ['earnings', 'eps', 'revenue', 'sales'],\n",
    "    'rating': ['rating', 'upgrade', 'downgrade', 'maintains', 'initiates'],\n",
    "    'market': ['stock', 'stocks', 'shares', 'market'],\n",
    "    'performance': ['high', 'low', 'gains', 'losses', 'beats', 'misses'],\n",
    "    'action': ['buy', 'sell', 'hold', 'analyst', 'analysts']\n",
    "}\n",
    "\n",
    "# Count occurrences of each category\n",
    "category_counts = {}\n",
    "for category, terms in financial_terms.items():\n",
    "    count = 0\n",
    "    for term in terms:\n",
    "        matching = [w for w, c in keywords if term in w.lower()]\n",
    "        if matching:\n",
    "            count += sum([c for w, c in keywords if term in w.lower()])\n",
    "    category_counts[category] = count\n",
    "\n",
    "print(\"\\n=== FINANCIAL TERM CATEGORIES ===\\n\")\n",
    "for category, count in sorted(category_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"{category.capitalize():15s}: {count:,} occurrences\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 6))\n",
    "categories = list(category_counts.keys())\n",
    "counts = list(category_counts.values())\n",
    "plt.bar(categories, counts, color='coral', edgecolor='black')\n",
    "plt.title('Financial Term Categories in Headlines', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Category')\n",
    "plt.ylabel('Occurrences')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2217fe7",
   "metadata": {
    "vscode": {
     "languageId": "code"
    }
   },
   "outputs": [],
   "source": [
    "# Analyze headline sentiment indicators\n",
    "sentiment_keywords = {\n",
    "    'positive': ['gains', 'high', 'upgrade', 'beats', 'outperform', 'buy', 'rises', 'up'],\n",
    "    'negative': ['losses', 'low', 'downgrade', 'misses', 'underperform', 'sell', 'falls', 'down'],\n",
    "    'neutral': ['maintains', 'hold', 'neutral', 'equal', 'peer']\n",
    "}\n",
    "\n",
    "# Count sentiment indicators\n",
    "sentiment_counts = {}\n",
    "for sentiment, terms in sentiment_keywords.items():\n",
    "    count = 0\n",
    "    for term in terms:\n",
    "        count += df['headline'].str.lower().str.contains(term, regex=False).sum()\n",
    "    sentiment_counts[sentiment] = count\n",
    "\n",
    "print(\"\\n=== SENTIMENT INDICATORS IN HEADLINES ===\\n\")\n",
    "for sentiment, count in sentiment_counts.items():\n",
    "    pct = (count / len(df)) * 100\n",
    "    print(f\"{sentiment.capitalize():10s}: {count:,} occurrences ({pct:.1f}% of articles)\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 6))\n",
    "colors_map = {'positive': 'green', 'neutral': 'gray', 'negative': 'red'}\n",
    "colors = [colors_map[s] for s in sentiment_counts.keys()]\n",
    "plt.bar(sentiment_counts.keys(), sentiment_counts.values(), color=colors, edgecolor='black', alpha=0.7)\n",
    "plt.title('Sentiment Indicators in Headlines', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Occurrences')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c20bdb",
   "metadata": {},
   "source": [
    "## 9. Summary and Key Findings\n",
    "\n",
    "Let's summarize the key insights from our EDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f884293",
   "metadata": {
    "vscode": {
     "languageId": "code"
    }
   },
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"EXPLORATORY DATA ANALYSIS - KEY FINDINGS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nüìä DATASET OVERVIEW\")\n",
    "print(f\"  ‚Ä¢ Total articles analyzed: {len(df):,}\")\n",
    "print(f\"  ‚Ä¢ Unique stocks covered: {df['stock'].nunique():,}\")\n",
    "print(f\"  ‚Ä¢ Unique publishers: {df['publisher'].nunique():,}\")\n",
    "print(f\"  ‚Ä¢ Date range: {df['date'].min().date()} to {df['date'].max().date()}\")\n",
    "\n",
    "print(\"\\nüìù HEADLINE CHARACTERISTICS\")\n",
    "print(f\"  ‚Ä¢ Average headline length: {df['headline_length'].mean():.1f} characters\")\n",
    "print(f\"  ‚Ä¢ Average word count: {df['word_count'].mean():.1f} words\")\n",
    "print(f\"  ‚Ä¢ Typical headline: {df['headline_length'].median():.0f} characters, {df['word_count'].median():.0f} words\")\n",
    "\n",
    "print(\"\\n‚úçÔ∏è PUBLISHER INSIGHTS\")\n",
    "top_pub = publisher_stats.iloc[0]\n",
    "print(f\"  ‚Ä¢ Most active publisher: {publisher_stats.index[0]}\")\n",
    "print(f\"  ‚Ä¢ Top 10 publishers: {top_10_pct:.1f}% of content\")\n",
    "print(f\"  ‚Ä¢ Publisher concentration: High (top 20 = {top_20_pct:.1f}%)\")\n",
    "\n",
    "print(\"\\nüìà STOCK COVERAGE\")\n",
    "top_stock = stock_stats.iloc[0]\n",
    "print(f\"  ‚Ä¢ Most covered stock: {stock_stats.index[0]} ({top_stock['article_count']:,} articles)\")\n",
    "print(f\"  ‚Ä¢ Top 50 stocks: {top_50_stocks_pct:.1f}% of coverage\")\n",
    "print(f\"  ‚Ä¢ Coverage distribution: Highly concentrated on major stocks\")\n",
    "\n",
    "print(\"\\nüìÖ TEMPORAL PATTERNS\")\n",
    "peak_hour = time_stats['hourly'].idxmax()\n",
    "peak_day = time_stats['day_of_week'].idxmax()\n",
    "print(f\"  ‚Ä¢ Peak publication hour: {peak_hour}:00 ({time_stats['hourly'][peak_hour]:,} articles)\")\n",
    "print(f\"  ‚Ä¢ Peak publication day: {day_names[peak_day]} ({time_stats['day_of_week'][peak_day]:,} articles)\")\n",
    "print(f\"  ‚Ä¢ Weekday vs Weekend: {weekend_counts[False]/len(df)*100:.1f}% weekday\")\n",
    "print(f\"  ‚Ä¢ Publication consistency: Active throughout business hours\")\n",
    "\n",
    "print(\"\\nüî§ CONTENT ANALYSIS\")\n",
    "print(f\"  ‚Ä¢ Top keywords: {', '.join([w for w, c in keywords[:5]])}\")\n",
    "print(f\"  ‚Ä¢ Common themes: Price targets, earnings, analyst ratings\")\n",
    "print(f\"  ‚Ä¢ Sentiment distribution:\")\n",
    "for sentiment, count in sentiment_counts.items():\n",
    "    print(f\"    - {sentiment.capitalize()}: {(count/len(df)*100):.1f}%\")\n",
    "\n",
    "print(\"\\nüí° KEY INSIGHTS\")\n",
    "print(\"  ‚Ä¢ News is heavily concentrated on large-cap, actively traded stocks\")\n",
    "print(\"  ‚Ä¢ Publications peak during market hours (pre-market and trading hours)\")\n",
    "print(\"  ‚Ä¢ Majority of content comes from a small number of publishers\")\n",
    "print(\"  ‚Ä¢ Headlines focus on price targets, ratings, and earnings\")\n",
    "print(\"  ‚Ä¢ Sentiment language is prevalent in headlines\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82a04c3",
   "metadata": {},
   "source": [
    "## 10. Export Processed Data\n",
    "\n",
    "Save the preprocessed data for use in subsequent tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de906c2",
   "metadata": {
    "vscode": {
     "languageId": "code"
    }
   },
   "outputs": [],
   "source": [
    "# Save processed data\n",
    "output_path = '../Data/processed_news_sample.csv'\n",
    "df.to_csv(output_path, index=False)\n",
    "print(f\"‚úÖ Processed data saved to: {output_path}\")\n",
    "\n",
    "print(f\"\\nüì¶ Saved {len(df):,} rows with {len(df.columns)} columns\")\n",
    "print(f\"Columns: {', '.join(df.columns.tolist())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffff8b91",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This EDA has provided comprehensive insights into the financial news dataset:\n",
    "\n",
    "1. **Data Quality**: Clean dataset with minimal missing values\n",
    "2. **Publisher Landscape**: Dominated by major financial news outlets\n",
    "3. **Stock Coverage**: Concentrated on high-profile stocks\n",
    "4. **Temporal Patterns**: Clear market-hours publication patterns\n",
    "5. **Content Themes**: Focus on ratings, price targets, and earnings\n",
    "\n",
    "**Next Steps for Task 2**: \n",
    "- Download historical stock price data for covered stocks\n",
    "- Calculate technical indicators (SMA, RSI, MACD)\n",
    "- Visualize price movements and indicators\n",
    "\n",
    "**Next Steps for Task 3**:\n",
    "- Perform sentiment analysis on headlines\n",
    "- Align news data with stock prices\n",
    "- Analyze correlation between sentiment and returns"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
